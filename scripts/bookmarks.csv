Level,PageNumber,Title
1,2,Contents
1,8,Website
1,9,Acknowledgments
1,12,Notation
1,16,Chapter 1 - Introduction
2,23,1.1 Who Should Read This Book?
2,26,1.2 Historical Trends in Deep Learning
3,28,1.2.1 The Many Names and Changing Fortunes of Neural Networks
3,34,1.2.2 Increasing Dataset Sizes
3,35,1.2.3 Increasing Model Sizes
3,38,"1.2.4 Increasing Accuracy, Complexity and Real-World Impact"
1,44,Part I - Applied Math and Machine Learning Basics
2,46,Chapter 2 - Linear Algebra
3,46,"2.1 Scalars, Vectors, Matrices and Tensors"
3,49,2.2 Multiplying Matrices and Vectors
3,51,2.3 Identity and Inverse Matrices
3,52,2.4 Linear Dependence and Span
3,54,2.5 Norms
3,55,2.6 Special Kinds of Matrices and Vectors
3,57,2.7 Eigendecomposition
3,59,2.8 Singular Value Decomposition
3,60,2.9 The Moore-Penrose Pseudoinverse
3,61,2.10 The Trace Operator
3,62,2.11 The Determinant
3,63,2.12 Example: Principal Components Analysis
2,68,Chapter 3 - Probability and Information Theory
3,69,3.1 Why Probability?
3,71,3.2 Random Variables
3,71,3.3 Probability Distributions
4,71,3.3.1 Discrete Variables and Probability Mass Functions
4,73,3.3.2 Continuous Variables and Probability Density Functions 
3,73,3.4 Marginal Probability
3,74,3.5 Conditional Probability
3,74,3.6 The Chain Rule of Conditional Probabilities
3,75,3.7 Independence and Conditional Independence
3,75,"3.8 Expectation, Variance and Covariance"
3,77,3.9 Common Probability Distributions
4,77,3.9.1 Bernoulli Distribution
4,77,3.9.2 Multinoulli Distribution
4,78,3.9.3 Gaussian Distribution
4,80,3.9.4 Exponential and Laplace Distributions
4,80,3.9.5 The Dirac Distribution and Empirical Distribution
4,81,3.9.6 Mixtures of Distributions
3,82,3.10 Useful Properties of Common Functions
3,85,3.11 Bayes&#8217; Rule
3,86,3.12 Technical Details of Continuous Variables
3,88,3.13 Information Theory
3,90,3.14 Structured Probabilistic Models
2,95,Chapter 4 - Numerical Computation
3,95,4.1 Overflow and Underflow
3,97,4.2 Poor Conditioning
3,97,4.3 Gradient-Based Optimization
4,101,4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices
3,108,4.4 Constrained Optimization
3,111,4.5 Example: Linear Least Squares
2,113,Chapter 5 - Machine Learning Basics
3,114,5.1 Learning Algorithms
4,114,"5.1.1 The Task, T"
4,118,"5.1.2 The Performance Measure, P"
4,119,"5.1.3 The Experience, E"
4,122,5.1.4 Example: Linear Regression
3,125,"5.2 Capacity, Overfitting and Underfitting"
4,131,5.2.1 The No Free Lunch Theorem
4,133,5.2.2 Regularization
3,135,5.3 Hyperparameters and Validation Sets
4,137,5.3.1 Cross-Validation
3,137,"5.4 Estimators, Bias and Variance"
4,137,5.4.1 Point Estimation
4,139,5.4.2 Bias
4,142,5.4.3 Variance and Standard Error
4,144,5.4.4 Trading off Bias and Variance to Minimize Mean Squared Error
4,145,5.4.5 Consistency
3,146,5.5 Maximum Likelihood Estimation
4,148,5.5.1 Conditional Log-Likelihood and Mean Squared Error
4,149,5.5.2 Properties of Maximum Likelihood
3,150,5.6 Bayesian Statistics
4,153,5.6.1 Maximum A Posteriori (MAP) Estimation
3,155,5.7 Supervised Learning Algorithms
4,155,5.7.1 Probabilistic Supervised Learning
4,156,5.7.2 Support Vector Machines
4,158,5.7.3 Other Simple Supervised Learning Algorithms
3,161,5.8 Unsupervised Learning Algorithms
4,162,5.8.1 Principal Components Analysis
4,165,5.8.2 k-means Clustering
3,166,5.9 Stochastic Gradient Descent
3,168,5.10 Building a Machine Learning Algorithm
3,170,5.11 Challenges Motivating Deep Learning
4,170,5.11.1 The Curse of Dimensionality
4,172,5.11.2 Local Constancy and Smoothness Regularization
4,176,5.11.3 Manifold Learning
1,181,Part II - Deep Networks: Modern Practices
2,183,Chapter 6 - Deep Feedforward Networks
3,186,6.1 Example: Learning XOR
3,192,6.2 Gradient-Based Learning
4,193,6.2.1 Cost Functions
5,193,6.2.1.1 Learning Conditional Distributions with Maximum Likelihood
5,195,6.2.1.2 Learning Conditional Statistics
4,196,6.2.2 Output Units
5,196,6.2.2.1 Linear Units for Gaussian Output Distributions
5,197,6.2.2.2 Sigmoid Units for Bernoulli Output Distributions
5,199,6.2.2.3 Softmax Units for Multinoulli Output Distributions
5,202,6.2.2.4 Other Output Types
3,206,6.3 Hidden Units
4,208,6.3.1 Rectified Linear Units and Their Generalizations
4,210,6.3.2 Logistic Sigmoid and Hyperbolic Tangent
4,211,6.3.3 Other Hidden Units
3,212,6.4 Architecture Design
4,213,6.4.1 Universal Approximation Properties and Depth
4,216,6.4.2 Other Architectural Considerations
3,219,6.5 Back-Propagation and Other Differentiation Algorithms
4,220,6.5.1 Computational Graphs
4,220,6.5.2 Chain Rule of Calculus
4,222,6.5.3 Recursively Applying the Chain Rule to Obtain Backprop
4,225,6.5.4 Back-Propagation Computation in Fully-Connected MLP
4,227,6.5.5 Symbol-to-Symbol Derivatives
4,230,6.5.6 General Back-Propagation
4,234,6.5.7 Example: Back-Propagation for MLP Training
4,236,6.5.8 Complications
4,236,6.5.9 Differentiation outside the Deep Learning Community
4,239,6.5.10 Higher-Order Derivatives
3,239,6.6 Historical Notes
2,243,Chapter 7 - Regularization for Deep Learning
3,245,7.1 Parameter Norm Penalties
4,246,7.1.1 L2 Parameter Regularization
4,249,7.1.2 L1 Regularization
3,252,7.2 Norm Penalties as Constrained Optimization
3,254,7.3 Regularization and Under-Constrained Problems
3,255,7.4 Dataset Augmentation
3,257,7.5 Noise Robustness
4,258,7.5.1 Injecting Noise at the Output Targets
3,258,7.6 Semi-Supervised Learning
3,259,7.7 Multi-Task Learning
3,261,7.8 Early Stopping
3,268,7.9 Parameter Tying and Parameter Sharing
3,269,7.10 Sparse Representations
3,271,7.11 Bagging and Other Ensemble Methods
3,273,7.12 Dropout
3,283,7.13 Adversarial Training
3,285,"7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier"
2,289,Chapter 8 - Optimization for Training Deep Models
3,290,8.1 How Learning Differs from Pure Optimization
4,290,8.1.1 Empirical Risk Minimization
4,291,8.1.2 Surrogate Loss Functions and Early Stopping
4,292,8.1.3 Batch and Minibatch Algorithms
3,297,8.2 Challenges in Neural Network Optimization
4,297,8.2.1 Ill-Conditioning
4,298,8.2.2 Local Minima
4,300,"8.2.3 Plateaus, Saddle Points and Other Flat Regions"
4,303,8.2.4 Cliffs and Exploding Gradients
4,304,8.2.5 Long-Term Dependencies
4,305,8.2.6 Inexact Gradients
4,306,8.2.7 Poor Correspondence between Local and Global Structure
4,308,8.2.8 Theoretical Limits of Optimization
3,309,8.3 Basic Algorithms
4,309,8.3.1 Stochastic Gradient Descent
4,311,8.3.2 Momentum
4,315,8.3.3 Nesterov Momentum
3,316,8.4 Parameter Initialization Strategies
3,321,8.5 Algorithms with Adaptive Learning Rates
4,322,8.5.1 AdaGrad
4,322,8.5.2 RMSProp
4,323,8.5.3 Adam
4,324,8.5.4 Choosing the Right Optimization Algorithm
3,325,8.6 Approximate Second-Order Methods
4,325,8.6.1 Newton&#8217;s Method
4,328,8.6.2 Conjugate Gradients
4,331,8.6.3 BFGS
3,332,8.7 Optimization Strategies and Meta-Algorithms
4,332,8.7.1 Batch Normalization
4,336,8.7.2 Coordinate Descent
4,337,8.7.3 Polyak Averaging
4,338,8.7.4 Supervised Pretraining
4,341,8.7.5 Designing Models to Aid Optimization
4,342,8.7.6 Continuation Methods and Curriculum Learning
2,345,Chapter 9 - Convolutional Networks
3,346,9.1 The Convolution Operation
3,350,9.2 Motivation
3,354,9.3 Pooling
3,360,9.4 Convolution and Pooling as an Infinitely Strong Prior
3,362,9.5 Variants of the Basic Convolution Function
3,373,9.6 Structured Outputs
3,375,9.7 Data Types
3,377,9.8 Efficient Convolution Algorithms
3,378,9.9 Random or Unsupervised Features
3,379,9.10 The Neuroscientific Basis for Convolutional Networks
3,386,9.11 Convolutional Networks and the History of Deep Learning
2,388,Chapter 10 - Sequence Modeling: Recurrent and Recursive Nets
3,390,10.1 Unfolding Computational Graphs
3,393,10.2 Recurrent Neural Networks
4,396,10.2.1 Teacher Forcing and Networks with Output Recurrence
4,399,10.2.2 Computing the Gradient in a Recurrent Neural Network
4,402,10.2.3 Recurrent Networks as Directed Graphical Models
4,406,10.2.4 Modeling Sequences Conditioned on Context with RNNs
3,409,10.3 Bidirectional RNNs
3,411,10.4 Encoder-Decoder Sequence-to-Sequence Architectures
3,413,10.5 Deep Recurrent Networks
3,415,10.6 Recursive Neural Networks
3,416,10.7 The Challenge of Long-Term Dependencies
3,419,10.8 Echo State Networks
3,421,10.9 Leaky Units and Other Strategies for Multiple Time Scales
4,422,10.9.1 Adding Skip Connections through Time
4,422,10.9.2 Leaky Units and a Spectrum of Different Time Scales
4,423,10.9.3 Removing Connections
3,423,10.10 The Long Short-Term Memory and Other Gated RNNs
4,425,10.10.1 LSTM
4,426,10.10.2 Other Gated RNNs
3,428,10.11 Optimization for Long-Term Dependencies
4,428,10.11.1 Clipping Gradients
4,430,10.11.2 Regularizing to Encourage Information Flow
3,431,10.12 Explicit Memory
2,436,Chapter 11 - Practical Methodology
3,437,11.1 Performance Metrics
3,440,11.2 Default Baseline Models
3,441,11.3 Determining Whether to Gather More Data
3,442,11.4 Selecting Hyperparameters
4,443,11.4.1 Manual Hyperparameter Tuning
4,447,11.4.2 Automatic Hyperparameter Optimization Algorithms
4,447,11.4.3 Grid Search
4,449,11.4.4 Random Search
4,450,11.4.5 Model-Based Hyperparameter Optimization
3,451,11.5 Debugging Strategies
3,455,11.6 Example: Multi-Digit Number Recognition
2,458,Chapter 12 - Applications
3,458,12.1 Large Scale Deep Learning
4,459,12.1.1 Fast CPU Implementations
4,459,12.1.2 GPU Implementations
4,462,12.1.3 Large Scale Distributed Implementations
4,462,12.1.4 Model Compression
4,463,12.1.5 Dynamic Structure
4,466,12.1.6 Specialized Hardware Implementations of Deep Networks
3,467,12.2 Computer Vision
4,468,12.2.1 Preprocessing
5,469,12.2.1.1 Contrast Normalization
5,472,12.2.1.2 Dataset Augmentation
3,473,12.3 Speech Recognition
3,476,12.4 Natural Language Processing
4,476,12.4.1 n-grams
4,478,12.4.2 Neural Language Models
4,480,12.4.3 High-Dimensional Outputs
5,481,12.4.3.1 Use of a Short List
5,482,12.4.3.2 Hierarchical Softmax
5,484,12.4.3.3 Importance Sampling
5,486,12.4.3.4 Noise-Contrastive Estimation and Ranking Loss
4,487,12.4.4 Combining Neural Language Models with n-grams
4,488,12.4.5 Neural Machine Translation
5,490,12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data
4,491,12.4.6 Historical Perspective
3,492,12.5 Other Applications
4,493,12.5.1 Recommender Systems
5,495,12.5.1.1 Exploration Versus Exploitation
4,497,"12.5.2 Knowledge Representation, Reasoning and Question Answering"
5,497,"12.5.2.1 Knowledge, Relations and Question Answering"
1,501,Part III - Deep Learning Research
2,504,Chapter 13 - Linear Factor Models
3,505,13.1 Probabilistic PCA and Factor Analysis
3,506,13.2 Independent Component Analysis (ICA)
3,508,13.3 Slow Feature Analysis
3,511,13.4 Sparse Coding
3,514,13.5 Manifold Interpretation of PCA
2,517,Chapter 14 - Autoencoders
3,518,14.1 Undercomplete Autoencoders
3,519,14.2 Regularized Autoencoders
4,520,14.2.1 Sparse Autoencoders
4,522,14.2.2 Denoising Autoencoders
4,523,14.2.3 Regularizing by Penalizing Derivatives
3,523,"14.3 Representational Power, Layer Size and Depth"
3,524,14.4 Stochastic Encoders and Decoders
3,525,14.5 Denoising Autoencoders
4,528,14.5.1 Estimating the Score
5,530,14.5.1.1 Historical Perspective
3,530,14.6 Learning Manifolds with Autoencoders
3,536,14.7 Contractive Autoencoders
3,538,14.8 Predictive Sparse Decomposition
3,539,14.9 Applications of Autoencoders
2,541,Chapter 15 - Representation Learning
3,543,15.1 Greedy Layer-Wise Unsupervised Pretraining
4,544,15.1.1 When and Why Does Unsupervised Pretraining Work?
3,551,15.2 Transfer Learning and Domain Adaptation
3,556,15.3 Semi-Supervised Disentangling of Causal Factors
3,561,15.4 Distributed Representation
3,568,15.5 Exponential Gains from Depth
3,569,15.6 Providing Clues to Discover Underlying Causes
2,573,Chapter 16 - Structured Probabilistic Models for Deep Learning
3,574,16.1 The Challenge of Unstructured Modeling
3,578,16.2 Using Graphs to Describe Model Structure
4,578,16.2.1 Directed Models
4,581,16.2.2 Undirected Models
4,583,16.2.3 The Partition Function
4,584,16.2.4 Energy-Based Models
4,587,16.2.5 Separation and D-Separation
4,588,16.2.6 Converting between Undirected and Directed Graphs
4,594,16.2.7 Factor Graphs
3,595,16.3 Sampling from Graphical Models
3,597,16.4 Advantages of Structured Modeling
3,597,16.5 Learning about Dependencies
3,598,16.6 Inference and Approximate Inference
3,599,16.7 The Deep Learning Approach to Structured Probabilistic Models
4,602,16.7.1 Example: The Restricted Boltzmann Machine
2,605,Chapter 17 - Monte Carlo Methods
3,605,17.1 Sampling and Monte Carlo Methods
4,605,17.1.1 Why Sampling?
4,606,17.1.2 Basics of Monte Carlo Sampling
3,607,17.2 Importance Sampling
3,610,17.3 Markov Chain Monte Carlo Methods
3,614,17.4 Gibbs Sampling
3,614,17.5 The Challenge of Mixing between Separated Modes
4,617,17.5.1 Tempering to Mix between Modes
4,618,17.5.2 Depth May Help Mixing
2,620,Chapter 18 - Confronting the Partition Function
3,621,18.1 The Log-Likelihood Gradient
3,622,18.2 Stochastic Maximum Likelihood and Contrastive Divergence
3,630,18.3 Pseudolikelihood
3,632,18.4 Score Matching and Ratio Matching
3,634,18.5 Denoising Score Matching
3,635,18.6 Noise-Contrastive Estimation
3,638,18.7 Estimating the Partition Function
4,640,18.7.1 Annealed Importance Sampling
4,643,18.7.2 Bridge Sampling
2,646,Chapter 19 - Approximate Inference
3,648,19.1 Inference as Optimization
3,649,19.2 Expectation Maximization
3,650,19.3 MAP Inference and Sparse Coding
3,653,19.4 Variational Inference and Learning
4,654,19.4.1 Discrete Latent Variables
4,660,19.4.2 Calculus of Variations
4,663,19.4.3 Continuous Latent Variables
4,665,19.4.4 Interactions between Learning and Inference
3,666,19.5 Learned Approximate Inference
4,666,19.5.1 Wake-Sleep
4,667,19.5.2 Other Forms of Learned Inference
2,669,Chapter 20 - Deep Generative Models
3,669,20.1 Boltzmann Machines
3,671,20.2 Restricted Boltzmann Machines
4,673,20.2.1 Conditional Distributions
4,674,20.2.2 Training Restricted Boltzmann Machines
3,675,20.3 Deep Belief Networks
3,678,20.4 Deep Boltzmann Machines
4,680,20.4.1 Interesting Properties
4,681,20.4.2 DBM Mean Field Inference
4,683,20.4.3 DBM Parameter Learning
4,684,20.4.4 Layer-Wise Pretraining
4,686,20.4.5 Jointly Training Deep Boltzmann Machines
3,691,20.5 Boltzmann Machines for Real-Valued Data
4,691,20.5.1 Gaussian-Bernoulli RBMs
4,693,20.5.2 Undirected Models of Conditional Covariance
3,698,20.6 Convolutional Boltzmann Machines
3,700,20.7 Boltzmann Machines for Structured or Sequential Outputs
3,701,20.8 Other Boltzmann Machines
3,702,20.9 Back-Propagation through Random Operations
4,704,20.9.1 Back-Propagating through Discrete Stochastic Operations
3,707,20.10 Directed Generative Nets
4,707,20.10.1 Sigmoid Belief Nets
4,708,20.10.2 Differentiable Generator Nets
4,711,20.10.3 Variational Autoencoders
4,715,20.10.4 Generative Adversarial Networks
4,718,20.10.5 Generative Moment Matching Networks
4,719,20.10.6 Convolutional Generative Networks
4,720,20.10.7 Auto-Regressive Networks
4,721,20.10.8 Linear Auto-Regressive Networks
4,722,20.10.9 Neural Auto-Regressive Networks
4,723,20.10.10 NADE
3,725,20.11 Drawing Samples from Autoencoders
4,726,20.11.1 Markov Chain Associated with any Denoising Autoencoder
4,726,20.11.2 Clamping and Conditional Sampling
4,728,20.11.3 Walk-Back Training Procedure
3,729,20.12 Generative Stochastic Networks
4,730,20.12.1 Discriminant GSNs
3,730,20.13 Other Generation Schemes
3,732,20.14 Evaluating Generative Models
3,734,20.15 Conclusion
1,736,Bibliography